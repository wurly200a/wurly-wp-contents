#+BLOG: wurly-blog
#+POSTID: 1389
#+ORG2BLOG:
#+DATE: [2024-06-17 Mon 07:47]
#+OPTIONS: toc:nil num:nil todo:nil pri:nil tags:nil ^:nil
#+CATEGORY: Kubernetes
#+TAGS: 
#+DESCRIPTION:
#+TITLE: HA ãŠã†ã¡Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æ§‹ç¯‰

* æ¦‚è¦

ã¤ã„ã«ã€æœ¬å½“ã®ï¼ŸHA ãŠã†ã¡Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æ§‹ç¯‰ã«å–ã‚Šçµ„ã¿ã¾ã™ã€‚

ä¸‹è¨˜ã§ã¯ã€ãƒ©ã‚ºãƒ‘ã‚¤3å°ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ï¼‹x86(amd64)ã®PC 1å°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’è¨­ã‘ã‚‹Kubernetes(K8S)ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚

 - [[./?p=1134][åŠHA(High availability) ãŠã†ã¡Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æ§‹ç¯‰]]

ã„ã–ä½¿ã„å§‹ã‚ãŸã¨ã“ã‚ã€ã“ã®æ§‹æˆã«ã¯ã„ã‚ã„ã‚ã¨èª²é¡ŒãŒã‚ã‚Šã¾ã—ãŸã€‚

K8Sã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å®Ÿç”¨çš„ã«ä½¿ç”¨ã™ã‚‹ã«ã¯K8Så†…ã®Load Balancerã‚µãƒ¼ãƒ“ã‚¹ã‚„ã€PV(Persist Volume)ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

Load Balancerã¯MetalLBã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã—ãŸãŒã€å¤–éƒ¨IPãŒå–å¾—ã§ããšã†ã¾ãå‹•ä½œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚

ã¾ãŸã€PVã«ã¤ã„ã¦ã‚‚ Longhornã€Rook Ceph ã®å°å…¥ã‚’è©¦ã¿ã¾ã—ãŸãŒã€ã“ã¡ã‚‰ã‚‚ã†ã¾ãè¡Œãã¾ã›ã‚“ã§ã—ãŸã€‚

ã„ãã¤ã‹åŸå› ãŒã‚ã‚Šãã†ãªã®ã§ã™ãŒã€CNIã‚’Calicoã‚’é¸æŠã—ãŸãŸã‚ã€MetalLB ã¨ã®ç›¸æ€§ãŒæ‚ªã„ã“ã¨ã‚„ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒ1å°ã—ã‹ç„¡ã„ãŸã‚ã€helmãƒãƒ£ãƒ¼ãƒˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®values.yamlã¨ã®æ§‹æˆãŒåˆã£ã¦ã„ãªã„ã‚ˆã†ã«è¦‹ãˆã¾ã—ãŸã€‚

ãã“ã§ã€ã‚„ã¯ã‚Šã¾ã£ã¨ã†ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’3å°è¨­ã‘ã‚‹æ§‹æˆã§æ§‹ç¯‰ã—ç›´ã™ã“ã¨ã«ã—ã¾ã—ãŸã€‚CNIã¨ã—ãŸã¯ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚’è€ƒæ…®ã—ã¦Ciliumã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

* æœ¬æŠ•ç¨¿ã®å†…å®¹ã«ã¤ã„ã¦

ãŠã†ã¡Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æ§‹ç¯‰ã«ã¤ã„ã¦ã¯ã€ä¸‹è¨˜ã®ã‚ˆã†ã«ã“ã‚Œã¾ã§ã„ã‚ã„ã‚ã¨è©¦ã—ã¦ãã¾ã—ãŸã€‚
æœ¬ç¨¿ã§ã¯é‡è¤‡ã™ã‚‹éƒ¨åˆ†ã¯å‰²æ„›ã—ã¦ã„ã¾ã™ã®ã§ã€å¿…è¦ã«å¿œã˜ã¦ä¸‹è¨˜ãƒšãƒ¼ã‚¸ã‚’å‚ç…§ãã ã•ã„ã€‚

 - ãƒ©ã‚ºãƒ‘ã‚¤1å°ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ï¼‹ãƒ©ã‚ºãƒ‘ã‚¤2å°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼
  - [[./?p=1011][ãŠã†ã¡Kubernetesã‚’ã¯ã˜ã‚ã‚‹(ãã®1)]] 
  - [[./?p=1035][ãŠã†ã¡Kubernetesã‚’ã¯ã˜ã‚ã‚‹(ãã®2)]]
  - [[./?p=1055][ãŠã†ã¡Kubernetesã‚’ã¯ã˜ã‚ã‚‹(ãã®3)]]
 - HAæ§‹æˆã«ã¤ã„ã¦
  - [[./?p=1090][Kubernetesã®HAæ§‹æˆ(ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã€etcdã€ãƒ¯ãƒ¼ã‚«ãƒ¼)]]
 - ãƒ©ã‚ºãƒ‘ã‚¤3å°ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ï¼‹PC 1å°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼
  - [[./?p=1134][åŠHA(High availability) ãŠã†ã¡Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æ§‹ç¯‰]]
 - ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
  - [[./?p=1132][ASUS Chromebox 3 ã« Ubuntu 22.04 ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]]
  - [[./?p=1326][Kubernetes x86-64(amd64) ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒã‚·ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—]]
 - ãƒ«ãƒ¼ã‚¿ãƒ¼ã€ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼
  - [[./?p=1260][WZR-1750DHP ã« OpenWrt ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]]
  - [[./?p=1312][HA Kubernetes ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å‘ã‘ haproxy ã®æ§‹ç¯‰]]
 - ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•
  - [[./?p=1081][kubectl ã§ãƒªãƒ¢ãƒ¼ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ã«æ¥ç¶š]]

* æ§‹æˆ

3å°ã®ãƒ©ã‚ºãƒ‘ã‚¤ã¨3å°ã®PC(Chromeboxæ”¹)ã€OpenWrtã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸ1å°ã®ãƒ«ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

ã¾ãŸã€å®¶ã®LANã®ãƒ«ãƒ¼ã‚¿ãƒ¼ã®LANå´IPã‚¢ãƒ‰ãƒ¬ã‚¹ã¯192.168.1.1ã§ã€LANã¯ 192.168.1.0/24 ã§ã™ãŒã€K8Sãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åˆ†ã‘ã€192.168.10.0/24 ã¨ã—ã¾ã—ãŸã€‚

# (æ§‹æˆã¨ã—ã¦ã¯é©åˆ‡ã§ã¯ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“)

ä»¥å‰ã®æ§‹æˆã§ã¯OpenWrtã‚’æœ‰ç·šãƒ–ãƒªãƒƒã‚¸ã¨ã—ã¦ã„ãŸã®ã§ã™ãŒã€ä»Šå›ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åˆ†ã‘ã¾ã—ãŸã€‚
ã“ã®æ–¹ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã—ã¦æœ‰åˆ©ã§ã™ã—ã€LBã‚„BGPãªã©ã‚’æ§‹æˆã™ã‚‹éš›ã«å•é¡Œã«ãªã‚Šã«ãã„ã€ã¾ãŸã¯å•é¡ŒãŒã‚ã£ã¦ã‚‚è§£æã—ã‚„ã™ã„ã¨è€ƒãˆã¾ã—ãŸã€‚
ã¾ãŸã€ä»¥å‰ã¯å„ãƒã‚·ãƒ³ã«ãŠã„ã¦Wi-Fiã‚’æœ‰åŠ¹ã«ã—ã¦ã„ãŸã®ã§ã™ãŒã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’æ§‹æˆã™ã‚‹éš›ã«ã¯ç„¡åŠ¹ã«ã—ã€æœ‰ç·šLANã®ã¿ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚

(ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åˆ†ã‘ãšã€Wi-Fiã‚’æœ‰åŠ¹ã«ã—ã¦ã„ãŸã®ã¯ã€å•é¡ŒãŒã‚ã£ãŸéš›ã«ä½œæ¥­ç”¨ã®PCã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®PCã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚„ã™ãã™ã‚‹ç›®çš„ã§ã—ãŸ)

 - 192.168.1.100/192.168.10.1 OpenWrt (BUFFALO WZR-1750DHP)
 - 192.168.10.11 k8s-ctrl1 (RaspberryPi 4B 8GB RAM)
 - 192.168.10.12 k8s-ctrl2 (RaspberryPi 4B 8GB RAM)
 - 192.168.10.13 k8s-ctrl3 (RaspberryPi 4B 8GB RAM)
 - 192.168.10.21 k8s-worker1 (ASUS Chromebox3 i7-8550U 16GB RAM, 256GB NVMe SSD)
 - 192.168.10.22 k8s-worker1 (ASUS Chromebox3 i7-8550U 16GB RAM, 256GB NVMe SSD)
 - 192.168.10.23 k8s-worker1 (ASUS Chromebox3 i7-8550U 16GB RAM, 256GB NVMe SSD)

* ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³

 - Ubuntu 22.04
 - Kubernetes v1.29 (v1.29.6)
 - Cilium v0.16.10

ä»Šå›æ§‹ç¯‰ã™ã‚‹ã®ã¯ä¸‹è¨˜ã®ã‚ˆã†ãªæ§‹æˆã¨ãªã‚Šã¾ã™ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼(192.168.1.100:6443) ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹(kubectrlã‚’å®Ÿè¡Œã™ã‚‹)ã“ã¨ã«ãªã‚Šã¾ã™ã€‚

#+begin_src mermaid :file images/1389_51.png
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': '#ffffff',
      'primaryTextColor': '#000000',
      'primaryBorderColor': '#000000',
      'lineColor': '#aaaaaa',
      'secondaryColor': '#ffffff',
      'tertiaryColor': '#aaaaaa'
    }
  }
}%%

graph TB
  subgraph controlplane3[control plane 3 192.168.10.13]
    APISERVER3["apiserver"]
    CONTROLLER3["controller-manager"]
    SCHEDULER3["scheduler"]
    ETCD3["etcd"]
  end

  subgraph controlplane2[control plane 2 192.168.10.12]
    APISERVER2["apiserver"]
    CONTROLLER2["controller-manager"]
    SCHEDULER2["scheduler"]
    ETCD2["etcd"]
  end

  subgraph controlplane1[control plane 1 192.168.10.11]
    APISERVER1["apiserver"]
    CONTROLLER1["controller-manager"]
    SCHEDULER1["scheduler"]
    ETCD1["etcd"]
  end
  
  LB["OpenWrt Router\nload balancer\n192.168.1.100:6443\n/192.168.10.1"]

  WORKER1["worker node\n192.168.10.21"]

  WORKER2["worker node\n192.168.10.22"]

  WORKER3["worker node\n192.168.10.23"]
  
  %% relation
  WORKER1---LB
  WORKER2---LB
  WORKER3---LB

  LB---APISERVER1
  LB---APISERVER2
  LB---APISERVER3

  APISERVER1---CONTROLLER1
  CONTROLLER1---SCHEDULER1
  SCHEDULER1---ETCD1

  APISERVER2---CONTROLLER2
  CONTROLLER2---SCHEDULER2
  SCHEDULER2---ETCD2

  APISERVER3---CONTROLLER3
  CONTROLLER3---SCHEDULER3
  SCHEDULER3---ETCD3

linkStyle 1 stroke:#000000
linkStyle 2 stroke:#000000
linkStyle 3 stroke:#000000
#+end_src

#+results:
[[file:images/1389_51.png]]

* ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ§‹æˆ

Rook Ceph ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã¤ã„ã¦ã¯ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’åˆ†ã‘ã¦ç©ºãã‚’ç¢ºä¿ã—ã¦ãŠãã¾ã™ã€‚

 - [[./?p=1399][Ubuntu 22.04 ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’åˆ†ã‘ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]]

* ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—(å€‹åˆ¥)

** k8s-ctrl1

#+begin_src bash
hostnamectl set-hostname k8s-ctrl1
#+end_src

#+begin_src bash
cat << _EOF_ | sudo tee -a /etc/netplan/01-netcfg.yaml
network:
  version: 2
  ethernets:
    eth0:
      addresses:
        - 192.168.10.11/24
      nameservers:
        addresses: [192.168.10.1]
      routes:
        - to: default
          via: 192.168.10.1
_EOF_
#+end_src

** k8s-ctrl2

#+begin_src bash
hostnamectl set-hostname k8s-ctrl2
#+end_src

#+begin_src bash
cat << _EOF_ | sudo tee -a /etc/netplan/01-netcfg.yaml
network:
  version: 2
  ethernets:
    eth0:
      addresses:
        - 192.168.10.12/24
      nameservers:
        addresses: [192.168.10.1]
      routes:
        - to: default
          via: 192.168.10.1
_EOF_
#+end_src

** k8s-ctrl3

#+begin_src bash
hostnamectl set-hostname k8s-ctrl3
#+end_src

#+begin_src bash
cat << _EOF_ | sudo tee -a /etc/netplan/01-netcfg.yaml
network:
  version: 2
  ethernets:
    eth0:
      addresses:
        - 192.168.10.13/24
      nameservers:
        addresses: [192.168.10.1]
      routes:
        - to: default
          via: 192.168.10.1
_EOF_
#+end_src

* ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—(å…±é€š)

** netplan

#+begin_src bash
sudo chmod 600 /etc/netplan/01-netcfg.yaml
sudo netplan apply
ip addr
#+end_src

#+begin_src bash
sudo apt update
sudo apt upgrade
sudo reboot
#+end_src

** etc/hosts

#+begin_src bash
cat << _EOF_ | sudo tee -a /etc/hosts
192.168.10.11  k8s-ctrl1
192.168.10.12  k8s-ctrl2
192.168.10.13  k8s-ctrl3
192.168.10.21  k8s-worker1
192.168.10.22  k8s-worker2
192.168.10.23  k8s-worker3
_EOF_
#+end_src

#+begin_src bash
cat /etc/hosts
#+end_src

** swap off

#+begin_src bash
sudo swapoff -a
sudo cat /etc/fstab | grep swap
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
#+end_src

** br_netfilter, overlay

#+begin_src bash
lsmod | grep -e br_netfilter -e overlay
#+end_src

#+begin_src bash
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
#+end_src

#+begin_src 
cat /etc/modules-load.d/containerd.conf
#+end_src

#+begin_src bash
sudo modprobe overlay
sudo modprobe br_netfilter
lsmod | grep -e br_netfilter -e overlay
#+end_src

** net.ipv4.ip_forward

#+begin_src bash
sysctl net.bridge.bridge-nf-call-ip6tables
sysctl net.bridge.bridge-nf-call-iptables
sysctl net.ipv4.ip_forward
#+end_src

#+begin_src bash
sudo sed -i 's/^#\(net.ipv4.ip_forward=1\)/\1/' /etc/sysctl.conf
cat /etc/sysctl.conf | grep ipv4.ip_forward
#+end_src

#+begin_src bash
sudo sysctl --system
#+end_src

#+begin_src bash
sysctl net.bridge.bridge-nf-call-ip6tables
sysctl net.bridge.bridge-nf-call-iptables
sysctl net.ipv4.ip_forward
#+end_src

** containerd

#+begin_src bash
sudo apt update
sudo apt install -y gnupg2
#+end_src

Note : "arm64" is for RaspberryPi

#+begin_src bash
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg
sudo add-apt-repository "deb [arch=arm64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
[Enter]
#+end_src

#+begin_src bash
sudo apt update
sudo apt install -y containerd.io
#+end_src

#+begin_src bash
containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1
cat /etc/containerd/config.toml | grep SystemdCgroup
sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
cat /etc/containerd/config.toml | grep SystemdCgroup
#+end_src

#+begin_src bash
sudo systemctl restart containerd
sudo systemctl status containerd
#+end_src

** kubernetes(kubelet,kubeadm,kubectl)

#+begin_src bash
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/trusted.gpg.d/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
#+end_src

#+begin_src bash
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
#+end_src

** linux-modules-extra-raspi

- [[https://docs.cilium.io/en/stable/operations/system_requirements/#ubuntu-22-04-on-raspberry-pi][System Requirements â€” Cilium 1.15.6 documentation]]

Cilium ã®å…¬å¼ã«ä¸‹è¨˜ã®ã‚ˆã†ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã—ãŸã®ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

#+begin_src 
Before running Cilium on Ubuntu 22.04 on a Raspberry Pi, please make sure to install the following package:
#+end_src

#+begin_src bash
sudo apt install linux-modules-extra-raspi
#+end_src

** disabled wi-fi

Comment out all the description in this file

#+begin_src bash
sudo vi /etc/netplan/50-cloud-init.yaml
#+end_src

#+begin_src bash
sudo reboot
#+end_src

* (ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã®å†—é•·åŒ–ã®ãŸã‚ã®)ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼(haproxy)

 - [[./?p=1260][WZR-1750DHP ã« OpenWrt ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]]
 - [[./?p=1312][HA Kubernetes ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å‘ã‘ haproxy ã®æ§‹ç¯‰]]

#+begin_src bash
# Default parameters
defaults
	# Default timeouts
	timeout connect 5000ms
	timeout client 50000ms
	timeout server 50000ms

listen kubernetes
	bind :6443

	option tcplog
    log global
    log 127.0.0.1 local0

	mode tcp

	balance roundrobin
	server k8s-ctrl1 192.168.1.11:6443 check fall 3 rise 2
	server k8s-ctrl2 192.168.1.12:6443 check fall 3 rise 2
	server k8s-ctrl3 192.168.1.13:6443 check fall 3 rise 2
#+end_src

#+begin_src bash
$ nc 127.0.0.1 6443
#+end_src

* kubeadm init (æœ€åˆã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³)

# ** k8s-ctrl1(å¤±æ•—)
# 
# #+begin_src bash
# # sudo kubeadm init --control-plane-endpoint "192.168.1.100:6443" --apiserver-advertise-address 192.168.10.1 --upload-certs
# #+end_src
# 
# #+begin_src bash
# wurly@k8s-ctrl1:~$ sudo kubeadm init --control-plane-endpoint "192.168.1.100:6443" --apiserver-advertise-address 192.168.10.1 --upload-certs
# I0620 08:40:48.380229    2063 version.go:256] remote version is much newer: v1.30.2; falling back to: stable-1.29
# [init] Using Kubernetes version: v1.29.6
# [preflight] Running pre-flight checks
# [preflight] Pulling images required for setting up a Kubernetes cluster
# [preflight] This might take a minute or two, depending on the speed of your internet connection
# [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
# W0620 08:41:30.587056    2063 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
# [certs] Using certificateDir folder "/etc/kubernetes/pki"
# [certs] Generating "ca" certificate and key
# [certs] Generating "apiserver" certificate and key
# [certs] apiserver serving cert is signed for DNS names [k8s-ctrl1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.1 192.168.1.100]
# [certs] Generating "apiserver-kubelet-client" certificate and key
# [certs] Generating "front-proxy-ca" certificate and key
# [certs] Generating "front-proxy-client" certificate and key
# [certs] Generating "etcd/ca" certificate and key
# [certs] Generating "etcd/server" certificate and key
# [certs] etcd/server serving cert is signed for DNS names [k8s-ctrl1 localhost] and IPs [192.168.10.1 127.0.0.1 ::1]
# [certs] Generating "etcd/peer" certificate and key
# [certs] etcd/peer serving cert is signed for DNS names [k8s-ctrl1 localhost] and IPs [192.168.10.1 127.0.0.1 ::1]
# [certs] Generating "etcd/healthcheck-client" certificate and key
# [certs] Generating "apiserver-etcd-client" certificate and key
# [certs] Generating "sa" key and public key
# [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
# [kubeconfig] Writing "admin.conf" kubeconfig file
# [kubeconfig] Writing "super-admin.conf" kubeconfig file
# [kubeconfig] Writing "kubelet.conf" kubeconfig file
# [kubeconfig] Writing "controller-manager.conf" kubeconfig file
# [kubeconfig] Writing "scheduler.conf" kubeconfig file
# [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
# [control-plane] Using manifest folder "/etc/kubernetes/manifests"
# [control-plane] Creating static Pod manifest for "kube-apiserver"
# [control-plane] Creating static Pod manifest for "kube-controller-manager"
# [control-plane] Creating static Pod manifest for "kube-scheduler"
# [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
# [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
# [kubelet-start] Starting the kubelet
# [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
# [kubelet-check] Initial timeout of 40s passed.
# 
# Unfortunately, an error has occurred:
#         timed out waiting for the condition
# 
# This error is likely caused by:
#         - The kubelet is not running
#         - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
# 
# If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
#         - 'systemctl status kubelet'
#         - 'journalctl -xeu kubelet'
# 
# Additionally, a control plane component may have crashed or exited when started by the container runtime.
# To troubleshoot, list all containers using your preferred container runtimes CLI.
# Here is one example how you may list all running Kubernetes containers by using crictl:
#         - 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
#         Once you have found the failing container, you can inspect its logs with:
#         - 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs CONTAINERID'
# error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
# To see the stack trace of this error execute with --v=5 or higher
# #+end_src
# 
# #+begin_src bash
#         - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
# #+end_src
# 
# ã¨ã„ã†ã“ã¨ãªã®ã§ã€
# 
# #+begin_src bash
# cat /etc/containerd/config.toml | grep SystemdCgroup
# sudo sed -i 's/SystemdCgroup \= true/SystemdCgroup \= false/g' /etc/containerd/config.toml
# cat /etc/containerd/config.toml | grep SystemdCgroup
# sudo systemctl restart containerd
# sudo systemctl status containerd
# #+end_src
# 
# #+begin_src bash
# sudo kubeadm reset --force
# #+end_src

#+begin_src bash
wurly@k8s-ctrl1:~$ nc localhost 6443 -v
nc: connect to localhost (127.0.0.1) port 6443 (tcp) failed: Connection refused
#+end_src

ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“ãŒã€èµ·å‹•ã—ã¦ã‹ã‚‰æ™‚é–“ãŒç«‹ã¤ã¨ Connection refused ã«ãªã£ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚Šã¾ã—ãŸã€‚
ãƒã‚·ãƒ³èµ·å‹•ç›´å¾Œã«å®Ÿè¡Œã—ã¾ã—ãŸã€‚

#+begin_src bash
sudo kubeadm init --control-plane-endpoint "192.168.1.100:6443" --upload-certs
#+end_src

#+begin_src bash
wurly@k8s-ctrl1:~$ sudo kubeadm init --control-plane-endpoint "192.168.1.100:6443" --upload-certs
[sudo] password for wurly: 
I0622 11:34:19.508444    1073 version.go:256] remote version is much newer: v1.30.2; falling back to: stable-1.29
[init] Using Kubernetes version: v1.29.6
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0622 11:35:06.098998    1073 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime
 is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-ctrl1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.11 192.168.1.100]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-ctrl1 localhost] and IPs [192.168.10.11 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-ctrl1 localhost] and IPs [192.168.10.11 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 17.041551 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
682be5e39e0c2b09b702dcc55eb63d9b186b46956948364b3b46b371cca2e0fb
[mark-control-plane] Marking the node k8s-ctrl1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-ctrl1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 9cdt7o.ac0zofa03n4cpywn
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.1.100:6443 --token 9cdt7o.ac0zofa03n4cpywn \
        --discovery-token-ca-cert-hash sha256:1b5aef58cb727699b774e4676457ff89bade426c52344fdc5891cd8de440a43e \
        --control-plane --certificate-key 682be5e39e0c2b09b702dcc55eb63d9b186b46956948364b3b46b371cca2e0fb

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.100:6443 --token 9cdt7o.ac0zofa03n4cpywn \
        --discovery-token-ca-cert-hash sha256:1b5aef58cb727699b774e4676457ff89bade426c52344fdc5891cd8de440a43e 
#+end_src

ã†ã¾ãè¡Œãã¾ã—ãŸã€‚

#+begin_src bash
sudo crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a
#+end_src

#+begin_src bash
wurly@k8s-ctrl1:~$ sudo crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
56ec3f62abd64       a75156450625c       About a minute ago   Running             kube-proxy                0                   75dadcce43ef2       kube-proxy-8pv9f
4601bde6bd1ed       46bfddf397d49       About a minute ago   Running             kube-apiserver            0                   43cdd65c1de77       kube-apiserver-k8s-ctrl1
51841ece14138       9df0eeeacdd8f       About a minute ago   Running             kube-controller-manager   0                   a2013a7bdb1e8       kube-controller-manager-k8s-ctrl1
8c69a45d684af       014faa467e297       About a minute ago   Running             etcd                      0                   94e9b436cfd92       etcd-k8s-ctrl1
3cf1ed4bbcf5c       4d823a436d04c       About a minute ago   Running             kube-scheduler            0                   8dc995dff5065       kube-scheduler-k8s-ctrl1
#+end_src

#+begin_src bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
#+end_src

* Cilium ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (æœ€åˆã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã®ã¿)

ä¸‹è¨˜ã®å…¬å¼æ‰‹é †é€šã‚Šã«ã‚„ã£ã¦ã¿ã¾ã™ã€‚

 - [[https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/#install-the-cilium-cli][Cilium Quick Installation â€” Cilium 1.15.6 documentation]]

#+begin_src bash
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
#+end_src

ä¸Šè¨˜ã®é€šã‚Šã‚„ã£ã¦ã‚‚å¤§ä¸ˆå¤«ã§ã™ãŒã€ãƒ©ã‚ºãƒ‘ã‚¤ãªã®ã§ã‚„ã‚‹ã¹ãã“ã¨ã¯ä¸‹è¨˜ã§ã™ã­ã€‚

#+begin_src bash
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=arm64
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
#+end_src

2024.6.22ç¾åœ¨ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ä¸‹è¨˜ã§ã—ãŸã€‚

#+begin_src bash
$ echo ${CILIUM_CLI_VERSION}
v0.16.10
#+end_src

#+begin_src bash
cilium install --version 1.15.6
#+end_src

#+begin_src bash
$ cilium install --version 1.15.6
â„¹ï¸  Using Cilium version 1.15.6
ğŸ”® Auto-detected cluster name: kubernetes
ğŸ”® Auto-detected kube-proxy has been installed
#+end_src

#+begin_src bash
wurly@k8s-ctrl1:~$ kubectl get pod -A
NAMESPACE     NAME                                READY   STATUS              RESTARTS   AGE
kube-system   cilium-lq477                        1/1     Running             0          31s
kube-system   cilium-operator-f45f4975f-cgzg6     1/1     Running             0          31s
kube-system   coredns-76f75df574-2p4fj            0/1     ContainerCreating   0          111m
kube-system   coredns-76f75df574-56zfw            0/1     ContainerCreating   0          111m
kube-system   etcd-k8s-ctrl1                      1/1     Running             0          111m
kube-system   kube-apiserver-k8s-ctrl1            1/1     Running             0          111m
kube-system   kube-controller-manager-k8s-ctrl1   1/1     Running             0          111m
kube-system   kube-proxy-8pv9f                    1/1     Running             0          111m
kube-system   kube-scheduler-k8s-ctrl1            1/1     Running             0          111m
#+end_src

#+begin_src bash
wurly@k8s-ctrl1:~$ kubectl get pod -A
NAMESPACE     NAME                                READY   STATUS    RESTARTS   AGE
kube-system   cilium-lq477                        1/1     Running   0          2m11s
kube-system   cilium-operator-f45f4975f-cgzg6     1/1     Running   0          2m11s
kube-system   coredns-76f75df574-2p4fj            1/1     Running   0          113m
kube-system   coredns-76f75df574-56zfw            1/1     Running   0          113m
kube-system   etcd-k8s-ctrl1                      1/1     Running   0          113m
kube-system   kube-apiserver-k8s-ctrl1            1/1     Running   0          113m
kube-system   kube-controller-manager-k8s-ctrl1   1/1     Running   0          113m
kube-system   kube-proxy-8pv9f                    1/1     Running   0          113m
kube-system   kube-scheduler-k8s-ctrl1            1/1     Running   0          113m
#+end_src

#+begin_src bash
wurly@k8s-ctrl1:~$ cilium status --wait
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    disabled (using embedded mode)
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet              cilium             Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium             Running: 1
                       cilium-operator    Running: 1
Cluster Pods:          2/2 managed by Cilium
Helm chart version:    
Image versions         cilium             quay.io/cilium/cilium:v1.15.6@sha256:6aa840986a3a9722cd967ef63248d675a87a                       cilium-operator    quay.io/cilium/operator-generic:v1.15.6@sha256:5789f0935eef96ad571e4f5565
#+end_src

* kubeadm init (ãã®ä»–ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³)

#+begin_src bash
sudo kubeadm init phase upload-certs --upload-certs
#+end_src

ä¸Šè¨˜ã§å†ç”Ÿæˆã—ãŸ certificate key ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

#+begin_src bash
sudo kubeadm join 192.168.1.100:6443 --token 9cdt7o.ac0zofa03n4cpywn \
        --discovery-token-ca-cert-hash sha256:1b5aef58cb727699b774e4676457ff89bade426c52344fdc5891cd8de440a43e \
        --control-plane --certificate-key 338f0272988bf66b6914994d28fb2ea6de4f399ce791f18aebf60ddd93e6c589
#+end_src

#+begin_src bash
$ k get nodes -o wide
NAME        STATUS   ROLES           AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
k8s-ctrl1   Ready    control-plane   153m    v1.29.6   192.168.10.11   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi   containerd://1.6.33
k8s-ctrl2   Ready    control-plane   2m55s   v1.29.6   192.168.10.12   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi   containerd://1.6.33
k8s-ctrl3   Ready    control-plane   79s     v1.29.6   192.168.10.13   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi   containerd://1.6.33
#+end_src

* worker

** k8s-worker1

#+begin_src yaml
cat << _EOF_ | sudo tee -a /etc/netplan/01-netcfg.yaml
network:
  version: 2
  wifis:
    wlp2s0:
      access-points:
        satori:
          password: e65601966fe93e613fd6fb970e4a5283240ef478a308ec9cc289e91d97bbc8a8
      dhcp4: true
      optional: true
  ethernets:
    eno0:
      addresses:
        - 192.168.10.21/24
      nameservers:
        addresses: [192.168.10.1]
      routes:
        - to: default
          via: 192.168.10.1
_EOF_
#+end_src

** k8s-worker2

#+begin_src yaml
cat << _EOF_ | sudo tee -a /etc/netplan/01-netcfg.yaml
network:
  version: 2
  wifis:
    wlp2s0:
      access-points:
        satori:
          password: e65601966fe93e613fd6fb970e4a5283240ef478a308ec9cc289e91d97bbc8a8
      dhcp4: true
      optional: true
  ethernets:
    eno0:
      addresses:
        - 192.168.10.22/24
      nameservers:
        addresses: [192.168.10.1]
      routes:
        - to: default
          via: 192.168.10.1
_EOF_
#+end_src

** k8s-worker3

#+begin_src yaml
cat << _EOF_ | sudo tee -a /etc/netplan/01-netcfg.yaml
network:
  version: 2
  wifis:
    wlp2s0:
      access-points:
        satori:
          password: e65601966fe93e613fd6fb970e4a5283240ef478a308ec9cc289e91d97bbc8a8
      dhcp4: true
      optional: true
  ethernets:
    eno0:
      addresses:
        - 192.168.10.23/24
      nameservers:
        addresses: [192.168.10.1]
      routes:
        - to: default
          via: 192.168.10.1
_EOF_
#+end_src

#+begin_src bash
sudo chmod 600 /etc/netplan/01-netcfg.yaml 
sudo rm /etc/netplan/00-installer-config*
sudo netplan apply
#+end_src

æœ‰ç·šå´ã®æ¥ç¶šãŒç¢ºèªã§ããŸã‚‰ã€ç„¡ç·šå´ã‚’ç„¡åŠ¹ã«ã—ã¾ã™ã€‚

#+begin_src bash
sudo free -m
#+end_src

#+begin_src bash
sudo swapoff -a
sudo sed -i '/swap/ s/^\(.*\)$/#\1/g' /etc/fstab
sudo cat /etc/fstab
#+end_src

#+begin_src bash
sudo reboot
#+end_src

#+begin_src bash
sudo free -m
#+end_src

#+begin_src bash
lsmod | grep -e br_netfilter -e overlay
#+end_src

#+begin_src bash
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
#+end_src


#+begin_src 
cat /etc/modules-load.d/containerd.conf
#+end_src

#+begin_src bash
sudo modprobe overlay
sudo modprobe br_netfilter
#+end_src


#+begin_src bash
lsmod | grep -e br_netfilter -e overlay
#+end_src

#+begin_src bash
sysctl net.bridge.bridge-nf-call-ip6tables
sysctl net.bridge.bridge-nf-call-iptables
sysctl net.ipv4.ip_forward
#+end_src

#+begin_src bash
sudo sed -i 's/^#\(net.ipv4.ip_forward=1\)/\1/' /etc/sysctl.conf
cat /etc/sysctl.conf | grep ipv4.ip_forward
#+end_src

#+begin_src bash
sudo sysctl --system
#+end_src

#+begin_src bash
sysctl net.bridge.bridge-nf-call-ip6tables
sysctl net.bridge.bridge-nf-call-iptables
sysctl net.ipv4.ip_forward
#+end_src

#+begin_src bash
sudo apt update
sudo apt install -y gnupg2
#+end_src

*æ³¨æ„* ä¸‹è¨˜ã¯ arch=amd64 ã‚’æŒ‡å®šã—ã¦ã„ã¾ã™ã€‚

#+begin_src bash
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
#+end_src

ã“ã®æ™‚ç‚¹ã§ã€ãƒªãƒã‚¸ãƒˆãƒªè¿½åŠ ã®ç¢ºèªã®ãŸã‚ã€Enterã‚­ãƒ¼å…¥åŠ›ã‚’ä¿ƒã•ã‚Œã‚‹å ´åˆã«ã¯ã€Enterã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦æ¬¡ã«é€²ã¿ã¾ã™ã€‚

#+begin_src bash
sudo apt update
sudo apt install -y containerd.io
#+end_src

#+begin_src bash
containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1
cat /etc/containerd/config.toml | grep SystemdCgroup
sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
cat /etc/containerd/config.toml | grep SystemdCgroup
#+end_src

#+begin_src bash
sudo systemctl restart containerd
sudo systemctl status containerd
#+end_src

#+begin_src bash
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/trusted.gpg.d/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
#+end_src

#+begin_src bash
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
#+end_src

** kubeadm join (k8s-worker1)

(æ³¨æ„) ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœŸé™åˆ‡ã‚Œã®å ´åˆã€controlãƒ—ãƒ¬ãƒ¼ãƒ³ã§å®Ÿè¡Œã—ã¾ã™ã€‚

#+begin_src bash
sudo kubeadm token list
sudo kubeadm token create
sudo kubeadm token list
#+end_src

workerã§å®Ÿè¡Œã—ã¾ã™ã€‚

#+begin_src bash
sudo kubeadm join 192.168.1.100:6443 --token (ä¸Šè¨˜ã§å–å¾—ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³) --discovery-token-ca-cert-hash sha256:1b5aef58cb727699b774e4676457ff89bade426c52344fdc5891cd8de440a43e --v=5
#+end_src

#+begin_src bash
$ k get node
NAME          STATUS     ROLES           AGE   VERSION
k8s-ctrl1     Ready      control-plane   27h   v1.29.6
k8s-ctrl2     Ready      control-plane   25h   v1.29.6
k8s-ctrl3     Ready      control-plane   25h   v1.29.6
k8s-worker1   NotReady   <none>          20s   v1.29.6
$ k get pods -A
NAMESPACE     NAME                                READY   STATUS     RESTARTS        AGE
kube-system   cilium-gdzzp                        1/1     Running    2 (3h48m ago)   25h
kube-system   cilium-j7z76                        0/1     Init:0/6   0               26s
kube-system   cilium-lq477                        1/1     Running    2 (3h48m ago)   26h
kube-system   cilium-operator-f45f4975f-cgzg6     1/1     Running    3 (3h48m ago)   26h
kube-system   cilium-rjtsm                        1/1     Running    2 (3h48m ago)   25h
kube-system   coredns-76f75df574-2p4fj            1/1     Running    2 (3h48m ago)   27h
kube-system   coredns-76f75df574-56zfw            1/1     Running    2 (3h48m ago)   27h
kube-system   etcd-k8s-ctrl1                      1/1     Running    2 (3h48m ago)   27h
kube-system   etcd-k8s-ctrl2                      1/1     Running    2 (3h48m ago)   25h
kube-system   etcd-k8s-ctrl3                      1/1     Running    2 (3h48m ago)   25h
kube-system   kube-apiserver-k8s-ctrl1            1/1     Running    2 (3h48m ago)   27h
kube-system   kube-apiserver-k8s-ctrl2            1/1     Running    2 (3h48m ago)   25h
kube-system   kube-apiserver-k8s-ctrl3            1/1     Running    2 (3h48m ago)   25h
kube-system   kube-controller-manager-k8s-ctrl1   1/1     Running    3 (3h48m ago)   27h
kube-system   kube-controller-manager-k8s-ctrl2   1/1     Running    2 (3h48m ago)   25h
kube-system   kube-controller-manager-k8s-ctrl3   1/1     Running    2 (3h48m ago)   25h
kube-system   kube-proxy-56bkk                    1/1     Running    0               26s
kube-system   kube-proxy-6rvft                    1/1     Running    2 (3h48m ago)   25h
kube-system   kube-proxy-8pv9f                    1/1     Running    2 (3h48m ago)   27h
kube-system   kube-proxy-kp95c                    1/1     Running    2 (3h48m ago)   25h
kube-system   kube-scheduler-k8s-ctrl1            1/1     Running    3 (3h48m ago)   27h
kube-system   kube-scheduler-k8s-ctrl2            1/1     Running    2 (3h48m ago)   25h
kube-system   kube-scheduler-k8s-ctrl3            1/1     Running    2 (3h48m ago)   25h
$ k get pods -A
NAMESPACE     NAME                                READY   STATUS    RESTARTS        AGE
kube-system   cilium-gdzzp                        1/1     Running   2 (3h55m ago)   25h
kube-system   cilium-j7z76                        1/1     Running   0               6m57s
kube-system   cilium-lq477                        1/1     Running   2 (3h55m ago)   26h
kube-system   cilium-operator-f45f4975f-cgzg6     1/1     Running   3 (3h55m ago)   26h
kube-system   cilium-rjtsm                        1/1     Running   2 (3h55m ago)   25h
kube-system   coredns-76f75df574-2p4fj            1/1     Running   2 (3h55m ago)   28h
kube-system   coredns-76f75df574-56zfw            1/1     Running   2 (3h55m ago)   28h
kube-system   etcd-k8s-ctrl1                      1/1     Running   2 (3h55m ago)   28h
kube-system   etcd-k8s-ctrl2                      1/1     Running   2 (3h55m ago)   25h
kube-system   etcd-k8s-ctrl3                      1/1     Running   2 (3h55m ago)   25h
kube-system   kube-apiserver-k8s-ctrl1            1/1     Running   2 (3h55m ago)   28h
kube-system   kube-apiserver-k8s-ctrl2            1/1     Running   2 (3h55m ago)   25h
kube-system   kube-apiserver-k8s-ctrl3            1/1     Running   2 (3h55m ago)   25h
kube-system   kube-controller-manager-k8s-ctrl1   1/1     Running   3 (3h55m ago)   28h
kube-system   kube-controller-manager-k8s-ctrl2   1/1     Running   2 (3h55m ago)   25h
kube-system   kube-controller-manager-k8s-ctrl3   1/1     Running   2 (3h55m ago)   25h
kube-system   kube-proxy-56bkk                    1/1     Running   0               6m57s
kube-system   kube-proxy-6rvft                    1/1     Running   2 (3h55m ago)   25h
kube-system   kube-proxy-8pv9f                    1/1     Running   2 (3h55m ago)   28h
kube-system   kube-proxy-kp95c                    1/1     Running   2 (3h55m ago)   25h
kube-system   kube-scheduler-k8s-ctrl1            1/1     Running   3 (3h55m ago)   28h
kube-system   kube-scheduler-k8s-ctrl2            1/1     Running   2 (3h55m ago)   25h
kube-system   kube-scheduler-k8s-ctrl3            1/1     Running   2 (3h55m ago)   25h
$ k get nodes -o wide
NAME          STATUS   ROLES           AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
k8s-ctrl1     Ready    control-plane   28h     v1.29.6   192.168.10.11   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi    containerd://1.6.33
k8s-ctrl2     Ready    control-plane   25h     v1.29.6   192.168.10.12   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi    containerd://1.6.33
k8s-ctrl3     Ready    control-plane   25h     v1.29.6   192.168.10.13   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi    containerd://1.6.33
k8s-worker1   Ready    <none>          7m13s   v1.29.6   192.168.10.21   <none>        Ubuntu 22.04.4 LTS   5.15.0-112-generic   containerd://1.6.33
#+end_src


** kubeadm joinã®å®Ÿè¡Œ (k8s-worker2)

ä¸Šè¨˜åŒæ§˜ã«å®Ÿæ–½ã—ã¾ã™ã€‚

** kubeadm joinã®å®Ÿè¡Œ (k8s-worker3)

ä¸Šè¨˜åŒæ§˜ã«å®Ÿæ–½ã—ã¾ã™ã€‚

* ç¢ºèª

ãƒ©ã‚ºãƒ‘ã‚¤3å°ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ï¼‹x86(amd64)ã®PC 3å°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’è¨­ã‘ã‚‹Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒæ§‹ç¯‰ã§ãã¾ã—ãŸã€‚

#+begin_src bash
$ k get nodes -o wide
NAME          STATUS   ROLES           AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
k8s-ctrl1     Ready    control-plane   14d    v1.29.6   192.168.10.11   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi    containerd://1.6.33
k8s-ctrl2     Ready    control-plane   14d    v1.29.6   192.168.10.12   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi    containerd://1.6.33
k8s-ctrl3     Ready    control-plane   14d    v1.29.6   192.168.10.13   <none>        Ubuntu 22.04.4 LTS   5.15.0-1055-raspi    containerd://1.6.33
k8s-worker1   Ready    <none>          13d    v1.29.6   192.168.10.21   <none>        Ubuntu 22.04.4 LTS   5.15.0-112-generic   containerd://1.6.33
k8s-worker2   Ready    <none>          4d8h   v1.29.6   192.168.10.22   <none>        Ubuntu 22.04.3 LTS   5.15.0-113-generic   containerd://1.7.18
k8s-worker3   Ready    <none>          4d8h   v1.29.6   192.168.10.23   <none>        Ubuntu 22.04.3 LTS   5.15.0-113-generic   containerd://1.7.18
#+end_src

* ãã®å¾Œ

ã“ã®ç’°å¢ƒã«å¯¾ã—ã€Rook ceph ã«ã‚ˆã‚‹ Storage ã®è¿½åŠ ã€Metallb ã«ã‚ˆã‚‹ Load Balancer ã®è¿½åŠ ã€Ingress-Nginx Controller ã®è¿½åŠ ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã—ãŸã€‚

ã“ã“ã¾ã§ãã‚Œã°å¤§æ¦‚ã®ã‚‚ã®ã¯ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã‚‹ã¯ãšã§ã™ã€‚

 - [[./?p=1371][Rook Ceph ã‚’ ãŠã†ã¡Kubernetesã«ãƒ‡ãƒ—ãƒ­ã‚¤]]
 - [[./?p=1531][Rook Ceph ã§ Storageã®ãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°]]

 - [[./?p=1376][Metallbã®æ¦‚è¦ã¨L2ãƒ¢ãƒ¼ãƒ‰ã§ã®ä½¿ã„æ–¹]]
 - [[./?p=1378][Metallb (BGPãƒ¢ãƒ¼ãƒ‰) ã¨ OpenWrt FRR ã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã™ã‚‹]]

 - [[./?p=1524][ãŠã†ã¡Kubernetes ã« Ingress-Nginx Controllerã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]]


# images/1389_51.png http://cha.la.coocan.jp/wp/wp-content/uploads/2024/07/1389_51.png
